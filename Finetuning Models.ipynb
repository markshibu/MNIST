{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Torchvision Models\n",
    "Two types of transfer learning on the pretrained [torchvision models](https://pytorch.org/docs/stable/torchvision/models.html)\n",
    "\n",
    "- finetuning\n",
    "  >start with a pretrained model\n",
    "\n",
    "  >update `all` of the model’s parameters for our new task, in essence retraining the whole model. \n",
    "\n",
    "- feature extraction\n",
    "  >start with a pretrained model\n",
    "  \n",
    "   >`only` update the final layer weights from which we derive predictions. More about transfer learning see [here](https://cs231n.github.io/transfer-learning/) and [here](https://ruder.io/transfer-learning/).\n",
    "\n",
    "In general both transfer learning methods follow the same few steps:\n",
    "\n",
    "-  `Initialize` the pretrained model\n",
    "-  `Reshape the final layer(s)` to have the same number of outputs as the number of classes in the new dataset\n",
    "-  `Define for the optimization` algorithm which parameters we want to update during training\n",
    "-  `Run` the training step\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "We will use the **[hymenoptera_data](https://download.pytorch.org/tutorial/hymenoptera_data.zip)** dataset\n",
    "\n",
    "This dataset contains two classes, **bees** and **ants**,\n",
    "and is structured such that we can use the [ImageFolder](https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder) dataset, \n",
    "rather than writing our own custom dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    \n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and Reshape the Networks\n",
    "**[Resnet](https://arxiv.org/abs/1512.03385)**\n",
    "> There are several variants of different sizes, including Resnet18, Resnet34, Resnet50, Resnet101, and Resnet152\n",
    "\n",
    "> Here we use `Resnet18`, as our dataset is small and only has two classes. \n",
    "\n",
    "> model.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "**[Alexnet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)**\n",
    "> The first very successful CNN on the ImageNet dataset\n",
    "\n",
    "> output comes from the 6th layer of the classifier\n",
    "\n",
    "> model.classifier[6] = nn.Linear(4096, num_classes)\n",
    "\n",
    "**[VGG](https://arxiv.org/pdf/1409.1556.pdf)**\n",
    "> Torchvision offers eight versions of VGG with various lengths and some that have batch normalizations layers\n",
    "\n",
    "> Here we use VGG-11 with batch normalization\n",
    "\n",
    "> model.classifier[6] = nn.Linear(4096, num_classes)\n",
    "\n",
    "**[Squeezenet](https://arxiv.org/abs/1602.07360)**\n",
    "> uses a different output structure than any of the other models shown here\n",
    "\n",
    "> Torchvision has two versions of Squeezenet, we use version 1.0\n",
    "\n",
    "> The output comes from a 1x1 convolutional layer which is the 1st layer of the classifier\n",
    "\n",
    "> model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "\n",
    "**[Densenet](https://arxiv.org/abs/1608.06993)**\n",
    "> Torchvision has four variants of Densenet\n",
    "\n",
    "> We use Densenet-121\n",
    "\n",
    "> model.classifier = nn.Linear(1024, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    model_ft = None\n",
    "    input_size = 224\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "    \n",
    "    return model_ft, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"data/hymenoptera_data\"\n",
    "model_name = \"resnet\"  # [resnet, alexnet, vgg, squeezenet, densenet]\n",
    "num_classes = 2\n",
    "batch_size = 8\n",
    "feature_extract = True\n",
    "input_size = 224\n",
    "\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "model_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "---------\n",
    "Initialize the data transforms, image datasets, and the dataloaders\n",
    "\n",
    "Notice, the models were pretrained with the hard-coded normalization values, as described [here](https://pytorch.org/docs/master/torchvision/models.html).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished initializing Datasets and Dataloaders\n"
     ]
    }
   ],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Finished initializing Datasets and Dataloaders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer\n",
    "--------------------\n",
    "\n",
    "Now that the model structure is correct, the final step for finetuning\n",
    "and feature extracting is to create an optimizer that only updates the\n",
    "desired parameters. Recall that after loading the pretrained model, but\n",
    "before reshaping, if ``feature_extract=True`` we manually set all of the\n",
    "parameter’s ``.requires_grad`` attributes to False. Then the\n",
    "reinitialized layer’s parameters have ``.requires_grad=True`` by\n",
    "default. So now we know that *all parameters that have\n",
    ".requires_grad=True should be optimized.* Next, we make a list of such\n",
    "parameters and input this list to the SGD algorithm constructor.\n",
    "\n",
    "To verify this, check out the printed parameters to learn. When\n",
    "finetuning, this list should be long and include all of the model\n",
    "parameters. However, when feature extracting this list should be short\n",
    "and only include the weights and biases of the reshaped layers.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "model_ft = model_ft.to(device)\n",
    "\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Validation Step\n",
    "--------------------------------\n",
    "\n",
    "Finally, the last step is to setup the loss for the model, then run the\n",
    "training and validation function for the set number of epochs. Notice,\n",
    "depending on the number of epochs this step may take a while on a CPU.\n",
    "Also, the default learning rate is not optimal for all of the models, so\n",
    "to achieve maximum accuracy it would be necessary to tune for each model\n",
    "separately.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 0.4080 Acc: 0.8279\n",
      "val Loss: 0.2694 Acc: 0.9085\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 0.3358 Acc: 0.8402\n",
      "val Loss: 0.2087 Acc: 0.9412\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 0.2825 Acc: 0.8934\n",
      "val Loss: 0.2222 Acc: 0.9216\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 0.3964 Acc: 0.8115\n",
      "val Loss: 0.2164 Acc: 0.9150\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 0.2958 Acc: 0.8852\n",
      "val Loss: 0.2355 Acc: 0.9216\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.2567 Acc: 0.9057\n",
      "val Loss: 0.2208 Acc: 0.9346\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.3100 Acc: 0.8811\n",
      "val Loss: 0.1941 Acc: 0.9346\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.2527 Acc: 0.8852\n",
      "val Loss: 0.2042 Acc: 0.9412\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.2733 Acc: 0.8852\n",
      "val Loss: 0.2943 Acc: 0.8954\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.3333 Acc: 0.8607\n",
      "val Loss: 0.2045 Acc: 0.9412\n",
      "\n",
      "Training complete in 2m 39s\n",
      "Best val Acc: 0.941176\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 10\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison with Model Trained from Scratch\n",
    "------------------------------------------\n",
    "\n",
    "Just for fun, lets see how the model learns if we do not use transfer\n",
    "learning. The performance of finetuning vs. feature extracting depends\n",
    "largely on the dataset but in general both transfer learning methods\n",
    "produce favorable results in terms of training time and overall accuracy\n",
    "versus a model trained from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 0.7830 Acc: 0.5123\n",
      "val Loss: 0.8654 Acc: 0.6209\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 0.7989 Acc: 0.4713\n",
      "val Loss: 0.6723 Acc: 0.6471\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 0.6835 Acc: 0.6393\n",
      "val Loss: 0.7482 Acc: 0.5686\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 0.6674 Acc: 0.6148\n",
      "val Loss: 0.5967 Acc: 0.6275\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 0.6531 Acc: 0.6352\n",
      "val Loss: 0.5571 Acc: 0.7255\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.6251 Acc: 0.6639\n",
      "val Loss: 0.7095 Acc: 0.6078\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.5993 Acc: 0.6680\n",
      "val Loss: 0.5590 Acc: 0.7190\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.5899 Acc: 0.6475\n",
      "val Loss: 0.6389 Acc: 0.6405\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.6680 Acc: 0.6803\n",
      "val Loss: 0.6150 Acc: 0.7451\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.5927 Acc: 0.7008\n",
      "val Loss: 0.6840 Acc: 0.6797\n",
      "\n",
      "Training complete in 2m 41s\n",
      "Best val Acc: 0.745098\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the non-pretrained version of the model used for this run\n",
    "scratch_model,_ = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False)\n",
    "scratch_model = scratch_model.to(device)\n",
    "scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9)\n",
    "scratch_criterion = nn.CrossEntropyLoss()\n",
    "_,scratch_hist = train_model(scratch_model, dataloaders_dict, scratch_criterion, scratch_optimizer, num_epochs=num_epochs)\n",
    "\n",
    "ohist = []\n",
    "shist = []\n",
    "\n",
    "ohist = [h.cpu().numpy() for h in hist]\n",
    "shist = [h.cpu().numpy() for h in scratch_hist]\n",
    "\n",
    "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "plt.plot(range(1,num_epochs+1),shist,label=\"Scratch\")\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
